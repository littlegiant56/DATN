import time
import json
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import StaleElementReferenceException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

chrome_options = Options()

chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1920,1080")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
chrome_options.add_experimental_option('useAutomationExtension', False)
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/113.0.0.0 Safari/537.36"
)

driver = webdriver.Chrome(options=chrome_options)
driver.get('https://www.vietnamworks.com/')
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.TAG_NAME, 'body'))
)
time.sleep(2)


def try_click(keyword):
    xpath = (
        f"//button[contains(normalize-space(text()), '{keyword}')"
        f" or contains(@aria-label, '{keyword}')]"
    )
    buttons = driver.find_elements(By.XPATH, xpath)
    if not buttons:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n√∫t n√†o v·ªõi t·ª´ kh√≥a: '{keyword}'")
        return
    for idx, btn in enumerate(buttons, start=1):
        try:
            driver.execute_script(
                "arguments[0].scrollIntoView({block: 'center'});", btn
            )
            time.sleep(0.2)
            btn.click()
            time.sleep(0.5)

        except StaleElementReferenceException:
            fresh = driver.find_elements(By.XPATH, xpath)
            if idx-1 < len(fresh):
                try:
                    btn2 = fresh[idx-1]
                    driver.execute_script(
                        "arguments[0].scrollIntoView({block: 'center'});", btn2
                    )
                    time.sleep(0.2)
                    btn2.click()
                    print(f"üîÑ Retry click n√∫t {idx} th√†nh c√¥ng")
                    time.sleep(1)
                except Exception as e2:
                    print(f"‚ùå Retry th·∫•t b·∫°i n√∫t {idx}: {e2}")
            else:
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m l·∫°i ƒë∆∞·ª£c n√∫t {idx} ƒë·ªÉ retry")
        except Exception as e:
            print(f"‚ùå L·ªói khi click n√∫t th·ª© {idx} '{keyword}': {e}")

def get_text_safe(xpath):
    try:
        labels = driver.find_elements(By.XPATH, xpath)
        if labels:
            sibling = labels[0].find_element(By.XPATH, "following-sibling::p")
            return sibling.text.strip()
    except:
        return None
    return None

def get_skills():
    try:
        p = driver.find_element(By.XPATH, "//label[text()='K·ª∏ NƒÇNG']/following-sibling::p")
        text = p.text.strip()
        skills = [s.strip() for s in text.split(",") if s.strip()]
        return skills
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y k√Ω nƒÉng: {e}")
        return None

def get_locations():
    try:
        # Gi·ªõi h·∫°n ph·∫°m vi t√¨m ki·∫øm: ch·ªâ l·∫•y c√°c <p name="paragraph"> trong kh·ªëi "ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác"
        location_elements = driver.find_elements(By.XPATH, "//h2[text()='ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác']/following::p[@name='paragraph']")
        locations = [el.text.strip() for el in location_elements if el.text.strip()]
        return ", ".join(locations)
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y ƒë·ªãa ƒëi·ªÉm: {e}")
        return None
    
def get_keywords():
    try:
        # X√°c ƒë·ªãnh ph·∫ßn b·∫Øt ƒë·∫ßu t·ª´ kh√≥a: ti√™u ƒë·ªÅ "T·ª´ kho√°:"
        keyword_block = driver.find_elements(By.XPATH, "//div[contains(text(),'T·ª´ kho√°')]")
        if not keyword_block:
            return None

        # T√¨m t·∫•t c·∫£ span trong button ph√≠a sau kh·ªëi "T·ª´ kho√°"
        keyword_buttons = driver.find_elements(By.XPATH, "//div[contains(@class, 'sc-a3652268-3')]//button/span")
        keywords = [el.text.strip() for el in keyword_buttons if el.text.strip()]
        return keywords
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y t·ª´ kh√≥a: {e}")
        return None
    
def crawl_job_details(url):
    driver.get(url)

    try:
        # B·∫•m c√°c n√∫t n·∫øu c√≥
        try_click("Xem ƒë·∫ßy ƒë·ªß m√¥ t·∫£ c√¥ng vi·ªác")
        try_click("Xem th√™m")
        time.sleep(1)

        # L·∫•y th√¥ng tin ch√≠nh
        title = driver.find_element(By.CSS_SELECTOR, "h1[name='title']").text
        company = driver.find_element(By.XPATH, "/html/body/main/div/main/div[2]/div/div/div/div[2]/div/div[1]/div[2]/a").text.strip()
        salary = driver.find_element(By.CSS_SELECTOR, "span[name='label']").text
        location = get_locations()
        job_description = driver.find_element(By.XPATH, "/html/body/main/div/main/div[2]/div/div/div/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div").text.strip()
        job_requirements = driver.find_element(By.XPATH, "/html/body/main/div/main/div[2]/div/div/div/div[1]/div/div[1]/div/div[3]/div/div/div[2]/div").text.strip()

        # L·∫•y th√¥ng tin th√™m
        data = {
            "title": title,
            "company": company,
            "salary": salary,
            "location": location,
            "job_description": job_description,
            "job_requirements": job_requirements,
            "date_posted": get_text_safe("//label[text()='NG√ÄY ƒêƒÇNG']"),
            "industry": get_text_safe("//label[text()='NG√ÄNH NGH·ªÄ']"),
            "field": get_text_safe("//label[text()='Lƒ®NH V·ª∞C']"),
            "experience": get_text_safe("//label[text()='S·ªê NƒÇM KINH NGHI·ªÜM T·ªêI THI·ªÇU']"),
            "education": get_text_safe("//label[text()='TR√åNH ƒê·ªò H·ªåC V·∫§N T·ªêI THI·ªÇU']"),
            "age": get_text_safe("//label[text()='ƒê·ªò TU·ªîI MONG MU·ªêN']"),
            "vacancy": get_text_safe("//label[text()='S·ªê L∆Ø·ª¢NG TUY·ªÇN D·ª§NG']"),
            "work_time": get_text_safe("//label[text()='GI·ªú L√ÄM VI·ªÜC']"),
            "level": get_text_safe("//label[text()='C·∫§P B·∫¨C']"),
            "skills": get_skills(),
            "language": get_text_safe("//label[text()='NG√îN NG·ªÆ TR√åNH B√ÄY H·ªí S∆†']"),
            "nationality": get_text_safe("//label[text()='QU·ªêC T·ªäCH']"),
            "gender": get_text_safe("//label[text()='GI·ªöI T√çNH']"),
            "marital_status": get_text_safe("//label[text()='T√åNH TR·∫†NG H√îN NH√ÇN']"),
            "work_date": get_text_safe("//label[text()='NG√ÄY L√ÄM VI·ªÜC']"),
            "work_type": get_text_safe("//label[text()='LO·∫†I H√åNH L√ÄM VI·ªÜC']"),
            "keywords": get_keywords()
        }
        return data

    except Exception as e:
        print(f"‚ùå L·ªói khi crawl job t·ª´ {url}: {e}")
        return None
    
def crawl_job_details_json(url):
    result = crawl_job_details(url)
    if result is None:
        return None
    return json.dumps(result, ensure_ascii=False, indent=4)

def close_driver():
    try:
        driver.quit()
    except Exception:
        pass

if __name__ == "__main__":
    url = "https://www.vietnamworks.com/technical-sales-manager-pharmaceutical-and-nutraceutical-ingredients-1909695-jv?source=searchResults&searchType=2&placement=1909695&sortBy=latest"
    json_str = crawl_job_details_json(url)
    if json_str:
        print(json_str)
    else:
        print("‚ùå Crawl th·∫•t b·∫°i, kh√¥ng c√≥ d·ªØ li·ªáu JSON")