import csv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import os 
from datetime import datetime, timedelta
from selenium.common.exceptions import StaleElementReferenceException

yesterday = datetime.now() #- timedelta(1)

input_csv_filename = "Rawdata/Links/" + yesterday.strftime("%d-%m-%Y") + "_links.csv"
output_csv_filename = yesterday.strftime("%d-%m-%Y") + "_jobs.csv"

# C·∫•u h√¨nh Chrome driver
chrome_options = Options()

# B·∫≠t ch·∫ø ƒë·ªô headless
chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1920,1080")

# Gi√∫p ch·∫°y ·ªïn ƒë·ªãnh tr√™n Linux/AWS
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# T·∫Øt c√°c c·∫£nh b√°o automation ƒë·ªÉ gi·∫£ l·∫≠p tr√¨nh duy·ªát th·∫≠t h∆°n
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
chrome_options.add_experimental_option('useAutomationExtension', False)

# Thi·∫øt l·∫≠p User-Agent gi·ªëng tr√¨nh duy·ªát th·∫≠t (b·∫°n c√≥ th·ªÉ thay user-agent ph√π h·ª£p)
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/113.0.0.0 Safari/537.36"
)

# Kh·ªüi t·∫°o tr√¨nh duy·ªát v·ªõi options ƒë√£ c·∫•u h√¨nh
driver = webdriver.Chrome(options=chrome_options)
driver.get('https://www.vietnamworks.com/')
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.TAG_NAME, 'body'))
)
time.sleep(2)

def read_job_links_from_csv(filename=input_csv_filename):
    job_links = []
    with open(filename, mode='r', encoding='utf-8') as file:
        for row in file:
            link = row.strip()
            if link and not link.lower().startswith('job link'):  # b·ªè header n·∫øu c√≥
                job_links.append(link)
    return job_links

# M·∫£ng c√°c link c·∫ßn crawl
job_links = read_job_links_from_csv(input_csv_filename)

# H√†m crawl th√¥ng tin c√¥ng vi·ªác
def try_click(keyword):
    xpath = (
        f"//button[contains(normalize-space(text()), '{keyword}')"
        f" or contains(@aria-label, '{keyword}')]"
    )
    buttons = driver.find_elements(By.XPATH, xpath)
    if not buttons:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n√∫t n√†o v·ªõi t·ª´ kh√≥a: '{keyword}'")
        return
    for idx, btn in enumerate(buttons, start=1):
        try:
            driver.execute_script(
                "arguments[0].scrollIntoView({block: 'center'});", btn
            )
            time.sleep(0.2)
            btn.click()
            time.sleep(0.5)

        except StaleElementReferenceException:
            fresh = driver.find_elements(By.XPATH, xpath)
            if idx-1 < len(fresh):
                try:
                    btn2 = fresh[idx-1]
                    driver.execute_script(
                        "arguments[0].scrollIntoView({block: 'center'});", btn2
                    )
                    time.sleep(0.2)
                    btn2.click()
                    print(f"üîÑ Retry click n√∫t {idx} th√†nh c√¥ng")
                    time.sleep(1)
                except Exception as e2:
                    print(f"‚ùå Retry th·∫•t b·∫°i n√∫t {idx}: {e2}")
            else:
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m l·∫°i ƒë∆∞·ª£c n√∫t {idx} ƒë·ªÉ retry")
        except Exception as e:
            print(f"‚ùå L·ªói khi click n√∫t th·ª© {idx} '{keyword}': {e}")
    try:
        buttons = driver.find_elements(By.XPATH, xpath)
        if buttons:
            driver.execute_script("arguments[0].scrollIntoView(true);", buttons[0])
            buttons[0].click()
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n√∫t: {xpath}")
    except Exception as e:
        print(f"‚ùå L·ªói khi b·∫•m n√∫t {xpath}: {e}")

# H√†m l·∫•y text kh√¥ng c·∫ßn wait
def get_text_safe(xpath):
    try:
        labels = driver.find_elements(By.XPATH, xpath)
        if labels:
            sibling = labels[0].find_element(By.XPATH, "following-sibling::p")
            return sibling.text.strip()
    except:
        return None
    return None

def get_locations():
    try:
        # Gi·ªõi h·∫°n ph·∫°m vi t√¨m ki·∫øm: ch·ªâ l·∫•y c√°c <p name="paragraph"> trong kh·ªëi "ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác"
        location_elements = driver.find_elements(By.XPATH, "//h2[text()='ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác']/following::p[@name='paragraph']")
        locations = [el.text.strip() for el in location_elements if el.text.strip()]
        return ", ".join(locations)
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y ƒë·ªãa ƒëi·ªÉm: {e}")
        return None
    
def get_keywords():
    try:
        # X√°c ƒë·ªãnh ph·∫ßn b·∫Øt ƒë·∫ßu t·ª´ kh√≥a: ti√™u ƒë·ªÅ "T·ª´ kho√°:"
        keyword_block = driver.find_elements(By.XPATH, "//div[contains(text(),'T·ª´ kho√°')]")
        if not keyword_block:
            return None

        # T√¨m t·∫•t c·∫£ span trong button ph√≠a sau kh·ªëi "T·ª´ kho√°"
        keyword_buttons = driver.find_elements(By.XPATH, "//div[contains(@class, 'sc-a3652268-3')]//button/span")
        keywords = [el.text.strip() for el in keyword_buttons if el.text.strip()]
        return ", ".join(keywords)
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y t·ª´ kh√≥a: {e}")
        return None

# H√†m crawl th√¥ng tin
def crawl_job_details(url):
    driver.get(url)

    try:
        # B·∫•m c√°c n√∫t n·∫øu c√≥
        try_click("Xem ƒë·∫ßy ƒë·ªß m√¥ t·∫£ c√¥ng vi·ªác")
        try_click("Xem th√™m")
        time.sleep(0.5)

        # L·∫•y th√¥ng tin ch√≠nh
        title = driver.find_element(By.CSS_SELECTOR, "h1[name='title']").text
        company = driver.find_element(By.XPATH, "/html/body/main/div/main/div[2]/div/div/div/div[2]/div/div[1]/div[2]/a").text.strip()
        salary = driver.find_element(By.CSS_SELECTOR, "span[name='label']").text
        location = get_locations()
        job_description = driver.find_element(By.XPATH, "/html/body/main/div/main/div[2]/div/div/div/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div").text.strip()
        job_requirements = driver.find_element(By.XPATH, "/html/body/main/div/main/div[2]/div/div/div/div[1]/div/div[1]/div/div[3]/div/div/div[2]/div").text.strip()

        # L·∫•y th√¥ng tin th√™m
        date_posted = get_text_safe("//label[text()='NG√ÄY ƒêƒÇNG']")
        industry = get_text_safe("//label[text()='NG√ÄNH NGH·ªÄ']")
        field = get_text_safe("//label[text()='Lƒ®NH V·ª∞C']")
        experience = get_text_safe("//label[text()='S·ªê NƒÇM KINH NGHI·ªÜM T·ªêI THI·ªÇU']")
        education = get_text_safe("//label[text()='TR√åNH ƒê·ªò H·ªåC V·∫§N T·ªêI THI·ªÇU']")
        age = get_text_safe("//label[text()='ƒê·ªò TU·ªîI MONG MU·ªêN']")
        vacancy = get_text_safe("//label[text()='S·ªê L∆Ø·ª¢NG TUY·ªÇN D·ª§NG']")
        work_time = get_text_safe("//label[text()='GI·ªú L√ÄM VI·ªÜC']")
        level = get_text_safe("//label[text()='C·∫§P B·∫¨C']")
        skills = get_text_safe("//label[text()='K·ª∏ NƒÇNG']")
        language = get_text_safe("//label[text()='NG√îN NG·ªÆ TR√åNH B√ÄY H·ªí S∆†']")
        nationality = get_text_safe("//label[text()='QU·ªêC T·ªäCH']")
        gender = get_text_safe("//label[text()='GI·ªöI T√çNH']")
        marital_status = get_text_safe("//label[text()='T√åNH TR·∫†NG H√îN NH√ÇN']")
        work_date = get_text_safe("//label[text()='NG√ÄY L√ÄM VI·ªÜC']")
        work_type = get_text_safe("//label[text()='LO·∫†I H√åNH L√ÄM VI·ªÜC']")
        keyword = get_keywords()

        return [title, company, salary, location, job_description, job_requirements, date_posted, industry, field,
                experience, education, age, vacancy, work_time, level, skills, language, nationality, gender,
                marital_status, work_date, work_type, keyword ]
    except Exception as e:
        print(f"‚ùå L·ªói khi crawl job t·ª´ {url}: {e}")
        return None

# L∆∞u v√†o CSV
def save_to_csv(data, filename=output_csv_filename):
    headers = ["Title", "Company", "Salary", "Location", "Job Description", "Job Requirements", "Date Posted", "Industry", "Field",
               "Experience", "Education", "Age", "Vacancy", "Work Time", "Level", "Skills", "Language",
               "Nationality", "Gender", "Marital Status", "Work Date", "Work Type", "Keyword"]
    
    file_exists = os.path.isfile(filename)
    
    with open(filename, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(headers)  # Ghi ti√™u ƒë·ªÅ n·∫øu file ch∆∞a c√≥
        writer.writerow(data)

# Th·ª±c thi crawl
counter = 0
for link in job_links:
    data = crawl_job_details(link)
    if data:
        save_to_csv(data)
        counter += 1
        print(f"‚úÖ ƒê√£ l∆∞u {counter} job.")

driver.quit()
